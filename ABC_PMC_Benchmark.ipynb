{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "#sns.set_style(\"whitegrid\")\n",
    "#sns.set_context(\"talk\")\n",
    "#rc('axes', labelsize=20, titlesize=20)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "from ABC_PMC import ABC_sample, ABC_PMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "# set up for the normal ABC example\n",
    "######\n",
    "\n",
    "prior_mean = -4.0\n",
    "prior_sd = 3\n",
    "likelihood_sd = 1\n",
    "original_data_size = 100\n",
    "\n",
    "def NormalPriorFunction(x):\n",
    "    return ss.norm.pdf(x=x,loc=prior_mean, scale=prior_sd)\n",
    "\n",
    "def NormalPriorSampler(n):\n",
    "    return np.random.normal(loc=prior_mean, scale=prior_sd, size=n)\n",
    "\n",
    "def NormalLiklihoodSimulator(n, param):\n",
    "    #unknown mean\n",
    "    return np.random.normal(loc=param, scale=likelihood_sd, size=n)\n",
    "    \n",
    "def NormalSummary(data):\n",
    "    return np.mean(data, axis=0)\n",
    "\n",
    "data = np.random.normal(loc=0,scale=likelihood_sd,size=original_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "requested_sample_size = np.linspace(start=50,stop=250,num=2)\n",
    "tolerance_seq = np.linspace(start=1, stop= 0.01, num=4) #T = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    niter  run_time  sample_size  times_1000_iter\n",
      "0   43026  0.760208         50.0         0.017658\n",
      "1  225077  3.897599        250.0         0.017312\n"
     ]
    }
   ],
   "source": [
    "ABC_benchmark = []\n",
    "\n",
    "for sample_size in requested_sample_size:\n",
    "    run_time_ABC = 0\n",
    "    niter_ABC = 0\n",
    "    times_for_iteration_ABC = 0\n",
    "    for rep in range(k):\n",
    "        #Run ABC\n",
    "        start_time = timeit.default_timer()\n",
    "        ABC_run = ABC_sample(NormalPriorSampler, NormalLiklihoodSimulator, NormalSummary, tolerance_seq[-1], data , sample_size)\n",
    "        run_time_ABC += timeit.default_timer() - start_time\n",
    "        niter_ABC += ABC_run[1]\n",
    "        times_for_iteration_ABC += np.mean(ABC_run[2])\n",
    "    run_time_ABC /= k\n",
    "    niter_ABC /= k\n",
    "    times_for_iteration_ABC /= k\n",
    "    output_dict = {'sample_size': sample_size, 'run_time': run_time_ABC, 'niter':niter_ABC, 'times_1000_iter': times_for_iteration_ABC}\n",
    "    ABC_benchmark.append(output_dict)\n",
    "\n",
    "file_name = \"data/ABC_benchmark_k_\"+str(k)+\"_tol_\"+str(tolerance_seq[-1])+\"_num_\"+str(len(requested_sample_size))+\".p\"\n",
    "pickle.dump(ABC_benchmark, open( file_name, \"wb\" ) ) #save result in a file\n",
    "#You can load using:\n",
    "#test = pickle.load( open( \"data/ABC_benchmark_k_2_tol_001.p\", \"rb\" ) )\n",
    "\n",
    "print(pd.DataFrame(ABC_benchmark))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   niter  run_time  sample_size  times_1000_iter\n",
      "0   2854  0.155979         50.0         0.048745\n",
      "1  13691  0.914394        250.0         0.068257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/numpy/core/numeric.py:190: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  a = empty(shape, dtype, order)\n"
     ]
    }
   ],
   "source": [
    "PMC_benchmark = []\n",
    "\n",
    "for sample_size in requested_sample_size:\n",
    "    run_time_PMC = 0\n",
    "    niter_PMC = 0\n",
    "    times_for_iteration_PMC = 0\n",
    "    for rep in range(k):\n",
    "        #Run ABC\n",
    "        start_time = timeit.default_timer()\n",
    "        PMC_run = ABC_PMC(NormalPriorFunction,NormalPriorSampler, NormalLiklihoodSimulator, NormalSummary, tolerance_seq, data , sample_size)\n",
    "        run_time_PMC += timeit.default_timer() - start_time\n",
    "        niter_PMC += PMC_run[1]\n",
    "        times_for_iteration_PMC += np.mean(PMC_run[2])\n",
    "    run_time_PMC /= k\n",
    "    niter_PMC /= k\n",
    "    times_for_iteration_PMC /= k\n",
    "    output_dict = {'sample_size': sample_size, 'run_time': run_time_PMC, 'niter':niter_PMC, 'times_1000_iter': times_for_iteration_PMC}\n",
    "    PMC_benchmark.append(output_dict)\n",
    "    \n",
    "file_name = \"data/PMC_benchmark_k_\"+str(k)+\"_tol_\"+str(tolerance_seq[-1])+\"_num_\"+str(len(requested_sample_size))+\"_T_\"+str(len(tolerance_seq))+\".p\"\n",
    "pickle.dump(PMC_benchmark, open( file_name, \"wb\" ) ) #save result in a file\n",
    "#You can load using:\n",
    "#test = pickle.load( open( \"data/ABC_benchmark_k_2_tol_001.p\", \"rb\" ) )\n",
    "\n",
    "print(pd.DataFrame(PMC_benchmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PMC benchmark for different values of T but fixed n\n",
    "sample_size_seq = [100,250,500,1000]\n",
    "\n",
    "PMC_benchmark2 = []\n",
    "for sample_size in sample_size_seq:\n",
    "    for T in range(2,11):\n",
    "        tolerance_seq = np.linspace(start=1, stop= 0.01, num=T)\n",
    "        run_time_PMC = 0\n",
    "        niter_PMC = 0\n",
    "        times_for_iteration_PMC = 0\n",
    "        for rep in range(k):\n",
    "            #Run ABC\n",
    "            start_time = timeit.default_timer()\n",
    "            PMC_run = ABC_PMC(NormalPriorFunction,NormalPriorSampler, NormalLiklihoodSimulator, NormalSummary, tolerance_seq, data , sample_size)\n",
    "            run_time_PMC += timeit.default_timer() - start_time\n",
    "            niter_PMC += PMC_run[1]\n",
    "            times_for_iteration_PMC += np.mean(PMC_run[2])\n",
    "        run_time_PMC /= k\n",
    "        niter_PMC /= k\n",
    "        times_for_iteration_PMC /= k\n",
    "        output_dict = {'T':T, 'sample_size': sample_size, 'run_time': run_time_PMC, 'niter':niter_PMC, 'times_1000_iter': times_for_iteration_PMC}\n",
    "        PMC_benchmark2.append(output_dict)\n",
    "\n",
    "file_name = \"data/PMC_benchmark2_k_\"+str(k)+\"_tol_\"+str(tolerance_seq[-1])+\"_num_\"+str(len(sample_size_seq))+\"_T_2_10.p\"\n",
    "pickle.dump(PMC_benchmark, open( file_name, \"wb\" ) ) #save result in a file\n",
    "print(pd.DataFrame(PMC_benchmark2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
